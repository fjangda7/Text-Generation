import utilities 

VALID_PUNCTUATION = ['?', '.' , '!', ',', ':', ';']
END_OF_SENTENCE_PUNCTUATION = ['?', '.', '!']
ALWAYS_CAPITALIZE = ["I", "Montmorency", "George", "Harris", "J", "London", "Thames", "Liverpool", \
                     "Flatland", "", "Mrs", "Ms", "Mr", "William", "Samuel"]
BAD_CHARS = ['"', "(", ")", "{", "}", "[", "]", "_"]

def parse_story(file_name):  

    """  

    (list) --> list 

    This function takes a text file and returns a list in which all the bad
    characters defined in BAD_CHARS are removed.

    Example: 

    >>parse_story('test_text_parsing.txt')  

    ['the', 'code', 'should', 'handle', 'correctly', 'the', 'following', ':', 'white', 'space', '.', 'sequences', 'of', 'punctuation', 'marks', '?', '!', '!', 'periods', 'with', 'or', 'without', 'spaces', ':', 'a', '.', '.', 'a', '.', 'a', "don't", 'worry', 'about', 'numbers', 'like', '1', '.', '5', 'remove', 'capitalization']

    """

    ordered_list = []
    word = ""
    number = ""
    x = ""

    file = open(file_name, "a")
    file.write(" ")
    file.close()

    file = open(file_name, "r")
    line = file.readline()

    while line != "":

        for character in line: 

            ascii_value = ord(character)

            #is the character a lowerecase letter or an apostrophe? 
            if (ascii_value == 39) or (ascii_value >= 97 and ascii_value <= 122):
                #yes, so add to word
                word = word + character

            #is the character a capital letter? 
            elif (ascii_value <= 90 and ascii_value >= 65): 

                #yes, so change to lowercase and add to word
                word = word + chr(ascii_value + 32)

            else: 
                #add word to list
                if word != "": 
                    ordered_list.append(word)
                word = ""

            if (ascii_value <= 59 and ascii_value >= 48):
                #yes, so add to word
                number = number + character

            else: 
                #add word to list
                if number != "":
                    ordered_list.append(number)
                number = ""

            if ascii_value == ord("-"):
                x = x + character
            else: 
                if x != "":
                    ordered_list.append(x)
                x = ""

            #is the character a number or valid punctuation? 
            if (ascii_value == 63 or ascii_value == 46 or ascii_value == 33 or ascii_value == 44): 
                #yes, so add to list
                ordered_list.append(character)


        line = file.readline()

    file.close()
    return ordered_list

#parse_story('test_text_parsing.txt')  

def get_prob_from_count(count):

    """
    (list) --> list

    This function takes a list of the number of occurances of a token from the previous n_gram and returns a list of the derived probablities.

    Example: 

    >>>get_prob_from_count([10, 20, 40, 30])
    [0.1, 0.2, 0.4, 0.3]
    """

    sum = 0
    probability = 0
    probabilities = []

    for occurance in count: 
        sum += occurance

    for occurance in count: 
        probability = occurance / sum
        probabilities.append(probability)

    return probabilities


def build_ngram_counts(words, n): 
    """

    (list, int) --> dict

    This function takes a list of words obtained from the parse_story function and the size of the N-gram and returns a dictionary with N-grams as the key with corresponding values that contain the counts of the words that follow the n-gram. 

    Example: 
    
    >>>build_ngram_counts([‘the’, ‘child’, ‘will’, ‘go’, ‘out’, ‘to’, ‘play’, ‘,’, ‘and’, ‘the’, ‘child’, ‘can’, ‘not’, ‘be’, ‘sad’, ‘anymore’,‘.’], 2)

    {
        (‘the’, ‘child’): [[‘will’, ‘can’], [1, 1]],
        (‘child’, ‘will’): [[‘go’], [1]],
        (‘will’, ‘go’): [[‘out’], [1]],
        (‘go’, out’): [[‘to’], [1]],
        (‘out’, ‘to’): [[‘play’], [1]],
        (‘to’, ‘play’): [[‘,’], [1]],
        (‘play’, ‘,’): [[‘and’], [1]],
        (‘,’, ‘and’): [[‘the’], [1]],
        (‘and’, ‘the’): [[‘child’], [1]],
        (‘child’, ‘can’): [[‘not’], [1]],
        (‘can’, ‘not’): [[‘be’], [1]],
        (‘not’, ‘be’): [[‘sad’], [1]],
        (‘be’, ‘sad’): [[‘anymore’], [1]],
        (‘sad’, ‘anymore’): [[‘.’], [1]]
    }

    """

    number_of_words = len(words)
    lastword = []
    dictionary = {}
    list_of_tuples = []

    for item in range(number_of_words):
        temp = ()

        if item < (number_of_words - (n - 1) - 1):
            for i in range(n): 
                temp += (words[item + i],)

                if i == n-1:
                    lastword.append(words[item + i + 1])


        if temp != (): 
            list_of_tuples.append(temp)

    for item in range(len(list_of_tuples)):
        newList = []
        count = []
        if list_of_tuples[item] not in dictionary.keys(): 

            newList = [[lastword[item]], [1]]
            dictionary[list_of_tuples[item]] = newList

        else: 
            index = list_of_tuples.index(list_of_tuples[item])

            indices = [i for i, x in enumerate(list_of_tuples) if x == list_of_tuples[item]]
            for i in indices: 
                newList.append(lastword[i])

            for word in newList: 
                if newList.count(word) > 1: 
                    count.append(newList.count(word))
                    for sameword in range(newList.count(word) - 1):
                        newList.reverse()
                        newList.remove(word)
                        newList.reverse()
                else: 
                    count.append(1)

            dictionary.update({list_of_tuples[index] : [newList, count]})

    return dictionary

#build_ngram_counts(['the', 'child', 'will', 'go', 'out', 'to', 'play', ',', 'and', 'the', 'child', 'can', 'not', 'be', 'sad', 'anymore','.'], 2)

#build_ngram_counts(['the', 'child', 'will', 'the', 'child', 'can', 'the', 'child', 'will', 'the', 'child', 'may', 'go', 'home', '.'], 2)

def prune_ngram_counts(counts, prune_len):
    """
    (dict, int) --> dict

    This function takes a dictionary of which the keys are n-grams and the values are the words that follow the n-gram and their respective counts (result from build_ngram_counts), and the number of highest frequency words to keep. It returns a dictionary with the same keys, but the values contain only the words with high frequencies. 

    Example: 

    >>>ngram_counts = {(‘i’, ‘love’): [[‘js’, ‘py3’, ‘c’, ‘no’], [20, 20, 10, 2]], (‘u’, ‘r’): [[‘cool’, ‘nice’, ‘lit’, 'kind’], [8, 7, 5, 5]], ('toronto’, ‘is’): [[‘six’, ‘drake’], [2, 3]]}

    >>>prune_ngram_counts(ngram_counts, 3)

    {(‘i’, ‘love’): [[‘js’, ‘py3’, ‘c’], [20, 20, 10]], (‘u’, ‘r’): [[‘cool’, ‘nice’, ‘lit’, 'kind’], [8, 7, 5, 5]], ('toronto’, ‘is’): [[‘six’, ‘drake’], [2, 3]]}
    """

    finalWords = []
    finalCounts = []

    for key, value in counts.items(): 

        countCopy = value[1].copy()
        countCopy.sort()
        countCopy.reverse()

        if len(value[1]) >= prune_len: 

            for count in countCopy:

                if len(finalCounts) < prune_len: 
                    finalCounts.append(count)
                else: 
                    if count in finalCounts: 
                        finalCounts.append(count)
                    elif count > min(finalCounts): 
                        finalCounts.replace(finalCounts.index(min(finalCounts)), count)

            for count in finalCounts: 

                if finalCounts.count(count) > 1: 
                    indices= [i for i, x in enumerate(value[1]) if x == count]

                    for index in indices: 
                        finalWords.append(value[0][index])

                else: 
                    finalWords.append(value[0][value[1].index(count)])
            
            finalWords = list(dict.fromkeys(finalWords))

            counts.update({key : [finalWords, finalCounts]})
            finalWords = []
            finalCounts = []

    return counts


#ngram_counts = {('i', 'love'): [['js', 'py3', 'c', 'no'], [20, 20, 10, 2]], ('u', 'r'): [['cool', 'nice', 'lit', 'kind'], [8, 7, 5, 5]], ('toronto', 'is'): [['six', 'drake'], [2, 3]]}

#prune_ngram_counts(ngram_counts, 2)

def probify_ngram_counts(counts): 
    """
    (dict) --> dict

    This function takes a dictionary with N-grams as keys and a nested list of words and counts as the values. The function converts the counts into probabilities, and returns a dictionary with probabilities as the valuesn instead.  

    Example: 

    >>> ngram_counts = {(‘i’, ‘love’): [[‘js’, ‘py3’, ‘c’], [20, 20, 10]], (‘u’, ‘r’): [[‘cool’, ‘nice’, ‘lit’, 'kind’], [8, 7, 5, 5]], ('toronto’, ‘is’): [[‘six’, ‘drake’], [2, 3]]}

    >>> probify_ngram_counts(ngram_counts)
    {(‘i’, ‘love’): [[‘js’, ‘py3’, ‘c’], [0.4, 0.4, 0.2]], (‘u’, ‘r’): [[‘cool’, ‘nice’, ‘lit’, 'kind’], [0.32, 0.28, 0.2, 0.2]], ('toronto’, ‘is’): [[‘six’, ‘drake’], [0.4, 0.6]]}

    """


    for key, value in counts.items(): 

        probabilities = get_prob_from_count(value[1])

        counts.update({key : [value[0], probabilities]})

    return counts

#ngram_counts = {('i', 'love'): [['js', 'py3', 'c'], [20, 20, 10]], ('u', 'r'): [['cool', 'nice', 'lit', 'kind'], [8, 7, 5, 5]], ('toronto', 'is'): [['six', 'drake'], [2, 3]]}

#probify_ngram_counts(ngram_counts)


def build_ngram_model(words, n):
    """

    (list, int) --> dict

    This function takes a list of words and punctuation and the size of the N-gram desired. It returns a dictionary with N-grams of size n as keys, and a nested list of corresponding next words and probabilities as values. The corresponding next words in the returned dictionary appear in descending order of probability. 

    Example: 

    >>> words = [‘the’, ‘child’, ‘will’, ‘the’, ‘child’, ‘can’, ‘the’, ‘child’, ‘will’, ‘the’, ‘child’, ‘may’, ‘go’, ‘home’, ‘.’]

    >>> build_ngram_model(words, 2)
    { 
        (‘the’, ‘child’): [[‘will’, ‘can’, ‘may’], [0.5, 0.25, 0.25]],
        (‘child’, ‘will’): [[‘the’], [1.0]],
        (‘will’, ‘the’): [[‘child’], [1.0]],
        (‘child’, ‘can’): [[‘the’], [1.0]],
        (‘can’, ‘the’): [[‘child’], [1.0]],
        (‘child’, ‘may’): [[‘go’], [1.0]],
        (‘may’, ‘go’): [[‘home’], [1.0]],
        (‘go’, ‘home’): [[‘.’], [1.0]]
    }
    
    """

    counts = probify_ngram_counts(build_ngram_counts(words, n))

    newWords = []

    for key, value in counts.items(): 

        probabilitiesCopy = value[1].copy()
        probabilitiesCopy.sort()
        probabilitiesCopy.reverse()

        for probability in probabilitiesCopy: 
            if probabilitiesCopy.count(probability) > 1: 

                indices= [i for i, x in enumerate(value[1]) if x == probability]

                for index in indices: 
                    newWords.append(value[0][index])

            else: 
                newWords.append(value[0][value[1].index(probability)])
            
        newWords = list(dict.fromkeys(newWords))

        counts.update({key : [newWords, probabilitiesCopy]})
        newWords = []

    return counts


#words = ['the', 'child', 'will', 'the', 'child', 'can', 'the', 'child', 'will', 'the', 'child', 'may', 'go', 'home', '.']
#build_ngram_model(words, 2)


def gen_bot_list(ngram_model, seed, num_tokens = 0): 
    
    """
    (dict, tuple, int) --> list

    This function takes an ngram_model with n sized n-grams , a tuple of n tokens, and the largest number of tokens desired. The function returns a list representing the subsequent token following the previous token, with a maximum size of num_tokens. 

    Example: 

    >>> ngram_model = 
    {
        ('the', 'child'): [['will', 'can','may'], [0.5, 0.25, 0.25]], 
        ('child', 'will'): [['the'], [1.0]], 
        ('will', 'the'): [['child'], [1.0]], 
        ('child', 'can'): [['the'], [1.0]], 
        ('can', 'the'): [['child'], [1.0]], 
        ('child', 'may'): [['go'], [1.0]], 
        ('may', 'go'): [['home'], [1.0]], 
        ('go', 'home'): [['.'], [1.0]] 
    }

    >>> gen_bot_list(ngram_model, ('the', 'child'), 5)
        ['the', 'child', 'will', 'the', 'child']

    """
    newList = []

    if num_tokens >= len(seed): 
        newList = list(seed)
    else: 
        for i in range(num_tokens):
            newList.append(seed[i])

    for ngram in range(num_tokens - 1): 

        if ngram != (num_tokens - 2):

            currentNGram = []

            for i in range(len(seed)):
                currentNGram.append(newList[ngram + i])

            currentNGram = tuple(currentNGram)

            if currentNGram in ngram_model.keys(): 
                newList.append(utilities.gen_next_token(currentNGram, ngram_model))
            else: 
                return newList 

    return newList


#odel = {('the', 'child'): [['will', 'can', 'may'], [0.5, 0.25, 0.25]], ('child', 'will'): [['the'], [1.0]], ('will', 'the'): [['child'], [1.0]], ('child', 'can'): [['the'], [1.0]], ('can', 'the'): [['child'], [1.0]], ('child', 'may'): [['go'], [1.0]], ('may', 'go'): [['home'], [1.0]], ('go', 'home'): [['.'], [1.0]]}

#model2 = {('the', 'child', 'has'): [['will', 'can', 'may'], [0.5, 0.25, 0.25]], ('child', 'has', 'a'): [['the'], [1.0]], ('has', 'a', 'bunny'): [['child'], [1.0]], ('a', 'bunny', 'rabbit'): [['the'], [1.0]], ('bunny', 'rabbit', 'that'): [['child'], [1.0]], ('rabbit', 'that', 'eats'): [['go'], [1.0]], ('that', 'eats', 'cherry'): [['home'], [1.0]], ('eats', 'cherry', 'salad'): [['.'], [1.0]]}


#gen_bot_list(model, ('the', 'child'), 6)
#gen_bot_list(model, ('hello', 'world'))
#gen_bot_list(model, ('hello', 'world'), 5)
#gen_bot_list(model2, ('the', 'child', 'has'), 5)


def gen_bot_text(token_list, bad_author): 
    """
    (list, boolean) --> string

    This function takes a list of tokens and returns a string of text with respect to grammar rules based on the value of bad_author.

    Example: 
    >>> token_list = ['this', 'is', 'a', 'string', 'of', 'text', '.', 'which', 'needs', 'to', 'be', 'created', '.']

    >>> gen_bot_text(token_list, False)
    'This is a string of text. Which needs to be created.'

    """
    newString = ''
    string = ""
    finalString = ""
    temp = ''

    if bad_author: 
        for index in range(len(token_list)): 

            if index != (len(token_list) - 1): 
                finalString += token_list[index] + " "
            else: 
                finalString += token_list[index] 

    else: 
        for token in token_list:
            for word in ALWAYS_CAPITALIZE: 
                if token.capitalize() == word: 
                    temp = word
                else: 
                    temp = token
            if temp == '?' or temp == '.' or temp == '!' or temp == ',' or temp == ':' or temp ==';':
                string += str(temp)
            else: 
                string += " " + str(temp)

        for chrIndex in range(len(string)): 
            ascii_value = ord(string[chrIndex])
            previousLetter = ord(string[chrIndex - 2])

            
            if chrIndex == 1 or previousLetter == 63 or previousLetter == 46 or previousLetter == 33:
                newString += chr(ascii_value - 32)
            else: 
                newString += chr(ascii_value)

        for chrIndex in range(len(newString)): 
            if chrIndex < len(newString) - 1:
                finalString += newString[chrIndex + 1]


    #print(finalString)
    return finalString


#token_list = ['this', 'is', 'a', 'string', 'of', 'text', '.', 'which', 'needs', 'to', 'be', 'created', '.']
#gen_bot_text(token_list, False)

def write_story(file_name, text, title, student_name, author, year): 

    """
    (string, string, string, string, string, int) --> None

    This function takes a text file and writes the text to a new file with name file_name, using the arguments given. This function returns None, but creates a file. 

    Example: 

    >>>write_story('test_write_story_student.txt', text, 'Three Men in a Boat', 'Jerome K. Jerome', 'Jerome K. Jerome', 1889)

    (no output)

    """

    totalCharacters = len(text)

    file = open(file_name, "w")
    file.write("\n" * 10)
    file.write(title + ": " + str(year) + ", UNLEASHED\n")
    file.write(student_name + ", inspired by " + author + "\n")
    file.write("Copyright year published (" + str(year)  + "), publisher: EngSci press" + "\n")
    file.write("\n" * 17)
    
    x = 90 
    y = 0 
    z = 0
    pageCount = 1
    lineNumber = 0
    lines = []
    line = ""
    counter = 0
    ascii_value = 0
    after_ascii_value = 0
    newText = ""


    while y < totalCharacters: 

        z = 0 

        while z == 0: 

            if (y + x + 1) > totalCharacters: 
                for chr in range(totalCharacters - y):
                    line += (text[chr])
                y = totalCharacters
                z = 1
                
                if line[0] == "":
                    line[1:]
                lines.append(line)
            
            else: 
                ascii_value = ord(text[y + x + 1])
                if ascii_value == 32 or (ascii_value  <= 59 and ascii_value  >= 48) or (ascii_value == 63 or ascii_value == 46 or ascii_value == 33 or ascii_value == 44): 
                
                    if y != 0: 
                        x = x + 1

                    for chr in range(y , y + x):
                        line += (text[chr])
                    
                    y = y + x
                    x = 90
                    z = 1
                    if line[0] == "":
                        line[1:]
                    lines.append(line)
                
                else: 
                    x = x - 1

            line = ""

    while lineNumber < len(lines): 
        

        if (pageCount + 11) % 12 == 0: 

            file.write("CHAPTER " + str(int((pageCount + 11) / 12)) + "\n\n")

            if (lineNumber + 27) < len(lines):
                for i in range(lineNumber, lineNumber + 26):
                    file.write(lines[i] + '\n')
            else: 
                for i in range(lineNumber, len(lines)):
                    counter += 1
                    file.write(lines[i] + '\n')
                
                for i in range(27 - counter):
                    file.write("\n")
            
            file.write('\n')
            file.write(str(pageCount) + '\n')
            lineNumber += 26
            pageCount += 1
        else: 
            if lineNumber + 29 < len(lines):
                for i in range (lineNumber, lineNumber + 28):
                    file.write(lines[i] + '\n')
            else: 
                for i in range (lineNumber, len(lines)):
                    counter += 1
                    file.write(lines[i] + '\n')
                for i in range(29 - counter):
                    file.write('\n')
            file.write('\n')
            file.write(str(pageCount) + '\n')
            lineNumber += 28
            pageCount += 1

    file.close()

text = ' '.join(parse_story('308.txt'))

write_story('test_write_story_student.txt', text, 'Three Men in a Boat', 'Jerome K. Jerome', 'Jerome K. Jerome', 1889)

#if _name_ == ‘__main__’”: